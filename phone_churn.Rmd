---
title: "phone_churn"
output: html_notebook
---

```{r}
library(mlr)
library(ggplot2)
library(robustHD)
library(readr)
data <- read_csv("E:/bitbucket_warehouse/side_projects/phone_churn/dataset/ACMETelephoneABT.csv")
```

We call a dataset imbalanced when the target variable is dominated by one class. Always, we more care about the minority class(es) than the dominant (majority) class. When we train a model, the training aims to reduce the error rate. The trained model may reluctantly or over-conservatively classify any observation as the minority class. Adjusting the classfication threshold in order to create more minority predictions is a good fix, but more can be done. We can make the dataset less imbalanced so that the model really learn about the characteristics of the minority class observations.

This dataset is perfectly balanced as the company that provided this dataset had already undersampled the majority class (customer who stay), while keep all minority customer rows (churned customers). In reality, we may instead oversample the abnormal customers, while keep all normal customers. It is not necessary to make two classes 50% 50% balanced.

```{r}
table(data$churn)
```

# Data processing
Notice that only columns that need processing are mentioned.

### How many values are missing in each column?
```{r}
sapply(data, function(x) sum(is.na(x)))
```

### occupation
Too many NAs in occupation. Assign missing. No imputing.
```{r}
data$occupation[is.na(data$occupation)] = 'missing'
```

### Small fix
As of May 2018, createDummyFeatures, the function that will be used for dummification, has column naming problem. Need to change "true" to "yes", "false" to "no".
```{r}
data[data == 'true'] = 'yes'
data[data == 'false'] = 'no'
```

### Save a copy
```{r}
data_no_handling = data
data_no_handling$regionType[is.na(data_no_handling$regionType)] = 'missing'
```

### This is an unnecessary index column.
```{r}
data$customer = NULL
```

### regionType
```{r}
table(data$regionType)
```

```{r}
data$regionType[data$regionType == 'r'] = 'rural'
data$regionType[data$regionType == 's'] = 'suburban'
data$regionType[data$regionType == 't'] = 'town'
data$regionType[data$regionType == 'unknown'] = NA
```


### creditCard
```{r}
table(data$creditCard)
```

```{r}
data$creditCard[data$creditCard == 'f'] = 'false'
data$creditCard[data$creditCard == 'no'] = 'false'
data$creditCard[data$creditCard == 't'] = 'true'
data$creditCard[data$creditCard == 'yes'] = 'true'
```

### numHandsets

numHandsets is just one of numerical columns which require different handling for different models.

1. For Logistic Regression, we need to bin any value >= say 3 as 3 (as a bin). 
2. For Naive Bayes, naiveBayes from e1071 assumes that the numerical variable is normally distributed, but here numHandsets is exponentially distributed. Therefore, we can bin any value >= say 3 as a bin, and then dummify this column. For simplicity, we can also let e1071 naiveBayes put a normal distribution assumption on this exponential distribution.
3. For KNN, since value 3 is in the context more similar to 6 then to 0, we need to make value 0 further from other values, before normalising this column.
4. No need to do anything to this column if we use tree based models.

```{r}
table(data$numHandsets)
```

### currentHandsetPrice

Is zero currentHandsetPrice due to value being missing or customers getting free phones from mobile plans? Choosing the later is safer. We will not set zero as NA.

```{r}
table(data$currentHandsetPrice)
```

### age

```{r}
hist(data$age)
```

```{r}
data$age[data$age == 0] = NA
```


## Imputation

Imputer requires factors for non-numerical columns.
```{r}
data = as.data.frame(unclass(data), stringsAsFactors=TRUE)
```

See a whole list of available imputation learners by running the two lines in below.

* For numerical missing values:   listLearners("regr", properties = "missings")[c("class", "package")]
* For categorical missing values: listLearners("classif", properties = "missings")[c("class", "package")]

This will take 10 mins to run.
```{r}
imp = impute(data, 
             target = "churn", # Tell the algorithm not to use information in the target column for imputation.
             cols = list(
                 regionType = imputeLearner("classif.cforest"),
                 age = imputeLearner("regr.cforest")
             )
            )
```

Let's see how is the result?
The attempt to impute regionType seems to be a failure. Almost all missing values were assigned to "suburban".
```{r}
table(imp$data$regionType)
```

The attempt to impute age seems good.
```{r}
hist(imp$data$age)
```

Now imp$data is our new data.
Also, give up imputing regionType. Simply fill missing values in regionType by any string.
```{r}
data_imputed = imp$data

data_imputed$regionType = data$regionType
levels(data_imputed$regionType) = c(levels(data_imputed$regionType), 'missing')
data_imputed$regionType[is.na(data_imputed$regionType)] = 'missing'
```

## Now if we jump to run the "Dummify" chunk, the data will be ready for tree based models. Here we just need more processing steps to get it ready for KNN.

## Logging, a special handling for this example

We will log all exponentially distributed columns. The zero values will be seperated from other values, which will enable the distances considered in KNN to reflect the context.

This transformation still will not make the variable normal enough for Naive Bayes' assumption. 

```{r}
library(MASS)
x = log(data_imputed$currentHandsetPrice + 1)
fit <- fitdistr(x, "normal")
para <- fit$estimate
hist(x, prob = TRUE, breaks = 30)
curve(dnorm(x, para[1], para[2]), col = 2, add = TRUE)
```


```{r}
data_imputed$numHandsets = log(data_imputed$numHandsets)
data_imputed$currentHandsetPrice = log(data_imputed$currentHandsetPrice + 1)
data_imputed$avgOverBundleMins = log(data_imputed$avgOverBundleMins + 1)
data_imputed$avgRoamCalls = log(data_imputed$avgRoamCalls + 1)
data_imputed$avgReceivedMins = log(data_imputed$avgReceivedMins + 1)
data_imputed$avgOutCalls = log(data_imputed$avgOutCalls + 1)
data_imputed$avgInCalls = log(data_imputed$avgInCalls + 1)
data_imputed$peakOffPeakRatio = log(data_imputed$peakOffPeakRatio + 1)
data_imputed$avgDroppedCalls = log(data_imputed$avgDroppedCalls + 1)
data_imputed$lastMonthCustomerCareCalls = log(data_imputed$lastMonthCustomerCareCalls + 1)
data_imputed$numRetentionCalls = log(data_imputed$numRetentionCalls + 1)
data_imputed$numRetentionOffersAccepted = log(data_imputed$numRetentionOffersAccepted + 1)
data_imputed$newFrequentNumbers = log(data_imputed$newFrequentNumbers + 1)
```

## Standardisation

```{r}
numerical_col_index <- unlist(lapply(data_imputed, is.numeric))  # For all numerical columns
data_standardised = lapply(data_imputed[ , numerical_col_index], standardize) # Standardise it
```

Check if mean is close to 0 and standard deviation is 1.
```{r}
mean(data_standardised$currentHandsetPrice)
sd(data_standardised$currentHandsetPrice)
```

Combine the standardised numerical column dataframe with the rest, i.e. the categorical column dataframe.
```{r}
categorical_col_index = !numerical_col_index
data_standardised = cbind(data_standardised, data_imputed[ , categorical_col_index])
```


## Dummify

In this example, for each categorical variable column, for n factor levels there will be n - 1 dummy variables (the first factor level will be dropped).
```{r}
data_dummified = createDummyFeatures(data_standardised, target = 'churn', method = "reference", cols = NULL)
```






