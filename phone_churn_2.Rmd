---
title: "phone_churn_2"
output: html_notebook
---

# Split data
```{r}
set.seed(1234)
data_split = makeResampleInstance(desc = 'Holdout', 
                                  task = makeClassifTask(data = data_dummified,
                                                         target = 'churn'),
                                  split = 0.7,
                                  stratify = TRUE)
```

# Tune a model

### Decision Tree
Apart from the following process, in Week 8 Warm Up, function makeTuneWrapper provides more convenience.
```{r}
rdesc = makeResampleDesc("CV", iters = 5, stratify = TRUE)
task = makeClassifTask(data = data_dummified[data_split$train.inds[[1]],  ], target = 'churn')
lrn_tree = makeLearner(cl = "classif.rpart", predict.type ='prob')
ps_tree = makeParamSet(makeNumericParam('cp', lower = 0.001, upper = 0.03),
                  makeIntegerParam('maxdepth', lower = 2, upper = 30))
ctrl_tree = makeTuneControlRandom(maxit = 40L)
res_tree = tuneParams(learner = lrn_tree,
                      task = task,
                      resampling = rdesc,
                      par.set = ps_tree,
                      control = ctrl_tree,
                      show.info =FALSE)
```

### Optional: See if there is still potential to improve parameter values.
```{r}
psdata_tree = generateHyperParsEffectData(res_tree)
plotHyperParsEffect(psdata_tree, x = "iteration", y = "mmce.test.mean", plot.type = "line")
```

### Construct a learner with the tuned parameters.
```{r}
lrn_tree_tuned = setHyperPars(lrn_tree, par.vals = res_tree$x)
```

# Feature Selection
```{r}
ncol(data)
```

### Run spsa on tuned and not-tuned learner
```{r}
spsaMod_tree_tuned = spFeatureSelection(task, wrapper = lrn_tree_tuned, 
                              measure = auc, num.features.selected = 0)

spsaMod_tree = spFeatureSelection(task, wrapper = lrn_tree, 
                              measure = auc, num.features.selected = 0)
```

### Optional: Save the best features as JSON
```{r}
spsaFeatures_tree = list( tunedLearnerFeatures   = spsaMod_tree_tuned$features,
                     untunedLearnerFeatures = spsaMod_tree$features)

write(toJSON( spsaFeatures_tree, auto_unbox = TRUE, pretty = TRUE, factor = 'string'),
      'spsaFeatures_tree.txt')
```

### Extract the spsa task (with the set of reduced number of features)
```{r}
spsaTask_tree_tuned = spsaMod_tree_tuned$task.spfs
spsaBestModel_tree_tuned  = spsaMod_tree_tuned$best.model

spsaTask_tree = spsaMod_tree$task.spfs
spsaBestModel_tree = spsaMod_tree$best.model
```

### Importance ploting
```{r}
spFSR::getImportance( spsaMod_tree_tuned )
spFSR::getImportance( spsaMod_tree )

spFSR::plotImportance( spsaMod_tree_tuned )
spFSR::plotImportance( spsaMod_tree )
```

# Train models
Only need to train the model that was tuned but did not go through the feature selection process. No need to train the model given by SPSA, since the "best.model" object from SPSA has already been trained.

```{r}
m = mlr::train(learner = lrn_tree_tuned, task = task)
```

# Define the test data (the task)
```{r}
task_test = makeClassifTask(data = data_dummified[data_split$test.inds[[1]],  ], target = 'churn')
```

# Predict
```{r}
pred_on_test       = predict(object = m, task = task_test)
pred_on_test_spsa_tree_tuned = predict(object = spsaBestModel_tree_tuned, task = task_test)
pred_on_test_spsa_tree = predict(object = spsaBestModel_tree, task = task_test)
```

# Evaluation

We may get the confusion matrix by running 
calculateConfusionMatrix(pred_on_test), 
but more importantly, we need to get the ROC.
```{r}
d  = generateThreshVsPerfData(pred_on_test, measures = list(fpr, tpr))
d_spsa_tree_tuned = generateThreshVsPerfData(pred_on_test_spsa_tree_tuned, measures = list(fpr, tpr))
d_spsa_tree = generateThreshVsPerfData(pred_on_test_spsa_tree, measures = list(fpr, tpr))

p = plotROCCurves(d) + labs(title = 'ROC curve of the tuned rpart learner', x = "")
p_spsa_tree_tuned = plotROCCurves(d_spsa_tree_tuned) + 
  labs(title = 'ROC curve of the tuned rpart learner with SPSA', x = "")
p_spsa_tree = plotROCCurves(d_spsa_tree) + labs(title = 'ROC curve of the original rpart learner with SPSA')

library(cowplot)
plot_grid(p, p_spsa_tree_tuned, p_spsa_tree, align = 'v', ncol = 1) # Run this line in console to visualise better.
```

### Remark:

1. pred_on_test is based on the tuned learner
2. pred_on_test_spsa_tree_tuned is based on the tuned learner + spsa
3. pred_on_test_spsa_tree is based on the original learner + spsa
Tuning and running spsa yield the best result.

```{r}
performance(pred_on_test,  measures = auc)
performance(pred_on_test_spsa_tree_tuned, measures = auc)
performance(pred_on_test_spsa_tree, measures = auc)
```





