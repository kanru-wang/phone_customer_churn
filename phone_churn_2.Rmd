---
title: "phone_churn_2"
output: html_notebook
---

# Split data
```{r}
data_split = makeResampleInstance(desc = 'Holdout', 
                                  task = makeClassifTask(data = data_dummified,
                                                         target = 'churn'),
                                  split = 0.8,
                                  stratify = TRUE)
```

# Tune a model

### Decision Tree
Apart from the following process, in Week 8 Warm Up, function makeTuneWrapper provides more convenience.
```{r}
rdesc = makeResampleDesc("CV", iters = 5, stratify = TRUE)
task = makeClassifTask(data = data_dummified[data_split$train.inds[[1]],  ], target = 'churn')
lrn = makeLearner(cl = "classif.rpart", predict.type ='prob')
ps = makeParamSet(makeNumericParam('cp', lower = 0.002, upper = 0.1))
ctrl = makeTuneControlRandom(maxit = 30L)
res = tuneParams(learner = lrn,
                 task = task,
                 resampling = rdesc,
                 par.set = ps,
                 control = ctrl,
                 show.info =FALSE)
```

### Optional: See if there is still potential to improve parameter values.
```{r}
psdata = generateHyperParsEffectData(res)
plotHyperParsEffect(psdata, x = "iteration", y = "mmce.test.mean", plot.type = "line")
```

### Optional: Visualisation of the best parameter value. 
We only tune one parameter in this example. Usually there are more parameters to be tuned and visualisation is hard to generate.
```{r}
plot(psdata$data[,1], psdata$data$mmce.test.mean, 
     xlab = "parameter_value", ylab  ="mmce.test.mean")
```

### Construct a learner with the tuned parameters.
```{r}
tuned_lrn = setHyperPars(lrn, par.vals = res$x)
```

# Predit
```{r}
m = train(learner = tuned_lrn, task = task)
task_test = makeClassifTask(data = data_dummified[data_split$test.inds[[1]],  ], target = 'churn')
pred_on_test = predict(object = m, task = task_test)
```

# Evaluation

We may get the confusion matrix by running 
calculateConfusionMatrix(pred_on_test), 
but more importantly, we need to get the ROC.
```{r}
d = generateThreshVsPerfData(pred_on_test, measures = list(fpr, tpr))
plotROCCurves(d)
```

```{r}
performance(pred_on_test, measures = auc)
```



# Comparing models side by side
```{r}
lrns = list(
  makeLearner("classif.rpart",id ="decisionTree", predict.type ='prob'),
  makeLearner("classif.kknn",id ="KNN", predict.type ='prob', k = 3),
  makeLearner("classif.naiveBayes",id ="naiveBayes", predict.type ='prob')
  )
bmr = benchmark(lrns, task, rdesc)
```


